---
title: "Final Project"
author: "Chi Zhang"
date: "5/6/2020"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(foreach)
library(cluster)
library(corrplot)
library(plotly)
library(tidyverse)
library(GGally)
library(LICORS)  # for kmeans++
library(randomForest)
library(stringr)
library(pdp)
library(gamlr)
library(knitr)
library(gbm)
```

# Reading Streaming Music: Approaches to Analysis in 2019 Pop Music

This report was made by Chi Zhang, UTEID: cz6753.

## Overview

When you're stuck at home, music can provide a sense of order amid the coronavirus chaos. Streaming music is cheap or even free (in the case of Pandora and Spotify) and outpaces any physical format when it comes to ease and convenience. There is no denying that streaming music is now faced with the biggest development opportunity since streaming services accounted for nearly 80% of all music revenue in past 2019, in accordance to a year-end report from the RIAA. Therefore, it is of importance to understand user behavior and preference when interacting with streaming music services. To accomplish this objective, we collected the data of the 2019 popular music database from Pandora, one of the widely-used media service providers, and tried to build the best predictive model possible for streams of songs through both linear and non-linear methods for comprehensiveness. More than that, I also segmented the songs into five groups through unsupervised algorithms and estimated the popularity trend for various groups throughout the year. By doing so, the result of this project could help streaming music servers improve their playlist song recommendations so that better meet users' needs and provide the basis for the optimization of future development roadmap.


## Data Sources

```{r data_intro, echo=FALSE, warning = FALSE}
myurl <- "https://raw.githubusercontent.com/ChiZhang18/Chi-Zhang-exercise/master/data/sounddata_2019.csv"
sounddata_2019 <- read.csv(url(myurl), row.names=1)
sounddata_2019 = subset(sounddata_2019,is.na(sounddata_2019$valence)==FALSE)
sounddata_2019$key = sounddata_2019$key %>% as.factor()
temp = model.matrix( ~ key-1,sounddata_2019)
sounddata_2019 = cbind(sounddata_2019,temp)
sounddata_2019$time_signature = sounddata_2019$time_signature %>% as.factor()
temp = model.matrix( ~ time_signature-1,sounddata_2019)
sounddata_2019 = cbind(sounddata_2019,temp)
temp = model.matrix( ~ explicit-1,sounddata_2019)
sounddata_2019 = cbind(sounddata_2019,temp)
relseaseDuration = as.Date(sounddata_2019$releaseDate)
temp = c()
for (i in c(1:length(relseaseDuration))) {
  temp = c(temp,as.Date("2019/1/1", origin = "1990/1/1"))
}
relseaseDuration = as.numeric(as.Date(temp, origin = "1990/1/1")-relseaseDuration)
sounddata_2019 = cbind(sounddata_2019, relseaseDuration)
#delete uncategorized label "unused attributes"
Clean_data <- subset(sounddata_2019, select = -c(releaseDate, artist_name, album_name, explicit, is_local, name, popularity, key0, key, time_signature, time_signature1, explicitFALSE))
Clean_data$acousticness <- Clean_data$acousticness %>% as.numeric()
myurl <- "https://raw.githubusercontent.com/ChiZhang18/Chi-Zhang-exercise/master/data/sounddata_2019weekly.csv"
sounddata_2019_weekly <- read.csv(url(myurl))
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)
  
  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)
  
  numPlots = length(plots)
  
  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                     ncol = cols, nrow = ceiling(numPlots/cols))
  }
  
  if (numPlots==1) {
    print(plots[[1]])
    
  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
    
    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
      
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

The Pandora Top Spins Chart is a record chart ranking the 100 tracks that have been streamed the most over the course of the past week. It is published weekly by Pandora. From https://www.nextbigsound.com/charts/top-spins, we downloaded the weekly data of the top 100 songs in the US. The data in year 2019 gives us access to 1,502 different songs. Given Pandora's public API, I had access to the data on the song features, artists and album information. By using web crawler, I could extract those target data from the web page and store them in .csv format. In the end, the formal dateset ultimately contains the following variables(attached with corresonding descriptions):

<b>Table1: Variables Description</b>

<p align="center">
  <img width="700" height="900" src="https://github.com/ChiZhang18/Chi-Zhang-exercise/blob/master/Unnamed%20Plots/fp-description.jpg">
</p>

<p align="center">
  <img width="705" height="800" src="https://github.com/ChiZhang18/Chi-Zhang-exercise/blob/master/Unnamed%20Plots/fp-description1.jpg">
</p>

## Predictive Model Building

In this part I intended to build a predictive model for the streams of songs in 2019. I selected between linear regression and decision tree models, using methods such as stepwise selection, lasso regression and random forests to make sure the robustness of prediction results. 
For the first model, I started with the null model by regressing streams on one, followed by running stepwise selection within 25 song feature variables and eventually obtained the final model. 
For the second model, I began with the medium model by regressing streams on all other 25 variables, and used stepwise method to choose variables within all the 25 song features and their interactions. 
By doing so, the two selected models are shown below. Noted that I had 5 and 31 significant coefficients, respectively in the first and second model.


```{r model1and2, echo = FALSE, warning = FALSE}
#Streams as an dependent variable
#stepwise
null1 = lm(Streams~1, data=Clean_data)
medium1 = lm(Streams ~ ., data=Clean_data)
big1 = lm(Streams ~ (.)^2, data=Clean_data)
stepwise1 = step(null1, scope=formula(medium1), dir="both", trace = FALSE)
stepwise2 = step(medium1, scope=formula(big1), dir="both", trace = FALSE)
model1 = formula(stepwise1)
model2 = formula(stepwise2)
print("model 1: ")
model1
print("model 2: ")
model2
```

I then used the Lasso model to assemble the best predictive model possible for streams. When using this method to select two models above, I did not allow for the interaction terms in the third model, but included them in the forth model.

For the third model, viewed from the path plot below I could find that minimum AIC occurs at segment 8, where there are 6 coefficients in this model.


<b> Figure1: Pathplot of Lasso (Model 3) </b>

```{r pathplot3, echo = FALSE, warning = FALSE}
#Lasso
Stx1 = sparse.model.matrix(Streams ~ ., data=Clean_data)[,-1] 
Sty1 = Clean_data$Streams
Stlasso1 = gamlr(Stx1, Sty1, lambda.min.ratio=0.000001)
#plot(log(Stlasso1$lambda), AICc(Stlasso1))
#which.min(AICc(Stlasso1))
plot(Stlasso1)
Stbeta1 = coef(Stlasso1)
```

Thus, I used the model at the segment 8 and chose 6 coefficients. The specific model is shown below.

```{r model3, echo = FALSE, warning = FALSE}
p1 <- dimnames(Stbeta1)[[1]]
p2 <- c()
for (i in c(1:length(Stbeta1))){
  p2 <- c(p2, as.list(Stbeta1)[[i]])
}
model3 = c("Streams ~ ")
for (i in c(2:length(Stbeta1))){
  if (p2[i] != 0){
    if (model3 == "Streams ~ "){
      model3 = paste(model3, p1[i])
    }
    else{
      model3 = paste(model3,"+", p1[i])
    }
  }
}
model3 <- as.formula(model3)
print("model 3: ")
model3
```

For the forth model, viewed from the path plot below I could see that minimum AIC occurs at segment 5, where there are 8 coefficients in the model.

<b> Figure2: Pathplot of Lasso (Model 4) </b>

```{r pathplot4, echo = FALSE, warning = FALSE}
Stx2 = sparse.model.matrix(Streams ~ (.)^2, data=Clean_data)[,-1] 
Sty2 = Clean_data$Streams
Stlasso2 = gamlr(Stx2, Sty2, lambda.min.ratio=0.00000001)
#plot(log(Stlasso2$lambda), AICc(Stlasso2))
#which.min(AICc(Stlasso2))
plot(Stlasso2)
```

Thus, I used the model at the segment 5 and chose 8 coefficients. The specific model is shown below.

```{r model4, echo = FALSE, warning = FALSE}
Stbeta2 = coef(Stlasso2)
p1 <- dimnames(Stbeta2)[[1]]
p2 <- c()
for (i in c(1:length(Stbeta2))){
  p2 <- c(p2, as.list(Stbeta2)[[i]])
}
model4 = c("Streams ~ ")
for (i in c(2:length(Stbeta2))){
  if (p2[i] != 0){
    if (model4 == "Streams ~ "){
      model4 = paste(model4, p1[i])
    }
    else{
      model4 = paste(model4,"+", p1[i])
    }
  }
}
model4 <- as.formula(model4)
print("model 4: ")
model4
#optimal lambda
#log(Stlasso1$lambda[which.min(AICc(Stlasso1))])
#sum(Stbeta1!=0)
#log(Stlasso2$lambda[which.min(AICc(Stlasso2))])
#sum(Stbeta2!=0)
```

Afterwards, I used the decision tree models to assemble the best predictive model possible for streams. I tried the random forest model and the boosting model on the dataset, which gave me 2 non-linear models: the fifth model and the sixth model.

```{r tree, echo = FALSE, warning = FALSE}
#trees and random forests
# split into a training and testing set
N = nrow(Clean_data)
train_frac = 0.8
N_train = floor(train_frac*N)
N_test = N - N_train
train_ind = sample.int(N, N_train, replace=FALSE) %>% sort
Clean_data_train = Clean_data[train_ind,]
Clean_data_test = Clean_data[-train_ind,]
# 1. bagging:
rmse_forest = c()
for (K in c(1:27)){
  forest1 = randomForest(Streams ~ ., mtry=K, nTree=50, data=Clean_data_train)
  yhat_forest_test = predict(forest1, Clean_data_test)
  rmse_foresttemp = mean((Clean_data_test$Streams - yhat_forest_test)^2) %>% sqrt
  rmse_forest = c(rmse_forest, rmse_foresttemp)
}
a=which.min(rmse_forest)
model5=randomForest(Streams ~ ., mtry=a, nTree=50, data=Clean_data_train)
# 2. Boosting:
boost1 = gbm(Streams ~ ., data=Clean_data_train, 
             interaction.depth=2, n.trees=200, shrinkage=.05, distribution = "gaussian")
#plot(Streams ~ energy, data=Clean_data_train)
#points(predict(boost1, n.trees=500) ~ energy, data=Clean_data_train, pch=19, col='red')
model6 = boost1
```

<b>Table2: RMSE of Different Models</b>

```{r K_fold, echo = FALSE, warning = FALSE}
#k-fold cross validation
N = nrow(Clean_data)
# Create a vector of fold indicators
K = 10
fold_id = rep_len(1:K, N)  # repeats 1:K over and over again
fold_id = sample(fold_id, replace=FALSE) # permute the order randomly
step_err_save1 = rep(0, K)
step_err_save2 = rep(0, K)
lasso_err_save1 = rep(0, K)
lasso_err_save2 = rep(0, K)
bag_err_save = rep(0,K)
boost_err_save = rep(0,K)
for(i in 1:K) {
  train_set = which(fold_id != i)
  y_test = Clean_data$Streams[-train_set]
  step_model1 = lm(model1, data=Clean_data[train_set,])
  step_model2 = lm(model2, data=Clean_data[train_set,])
  lasso_model1 = lm(model3, data=Clean_data[train_set,])
  lasso_model2 = lm(model4, data=Clean_data[train_set,])
  bag_model = randomForest(Streams ~ ., mtry=a, nTree=100, data=Clean_data[train_set,])
  boost_model = gbm(Streams ~ ., data=Clean_data[train_set,], interaction.depth=2, n.trees=500, shrinkage=.05, distribution = "gaussian")
  yhat_test1 = predict(step_model1, newdata=Clean_data[-train_set,])
  step_err_save1[i] = mean((y_test - yhat_test1)^2)
  yhat_test2 = predict(step_model2, newdata=Clean_data[-train_set,])
  step_err_save2[i] = mean((y_test - yhat_test2)^2)
  yhat_test3 = predict(lasso_model1, newdata=Clean_data[-train_set,])
  lasso_err_save1[i] = mean((y_test - yhat_test3)^2)
  yhat_test4 = predict(lasso_model2, newdata=Clean_data[-train_set,])
  lasso_err_save2[i] = mean((y_test - yhat_test4)^2)
  yhat_test5 = predict(bag_model, newdata=Clean_data[-train_set,])
  bag_err_save[i] = mean((y_test - yhat_test5)^2)
  yhat_test6 = predict(boost_model, newdata=Clean_data[-train_set,], n.trees=500)
  boost_err_save[i] = mean((y_test - yhat_test6)^2)
}
# RMSE
t1 = c("Model 1","Model 2","Model 3","Model 4","Model 5","Model 6" )
t2 = c(sqrt(mean(step_err_save1)), sqrt(mean(step_err_save2)), sqrt(mean(lasso_err_save1)), sqrt(mean(lasso_err_save2)), sqrt(mean(bag_err_save)), sqrt(mean(boost_err_save)))
table1 = cbind(t1,t2)
colnames(table1)<- c("Model", "CV")
kable(table1)
```

Lastly, I used k-fold cross validation in order to compare all 6 models above. I found that the CVs of the second model has the minimum CV, and therefore it is the best predictive model possible for streams. The advantage of a linear model is that a linear model with interactions is much easier to interpret than the non-linear models.

The second best model was the fifth model, which came from the random forest method. The random forest model has one advantage over the linear regression: it will only give us positive predictions. As a result, I used both the second model and the fifth model to do the predictions.


<b>Table3: Coefficients of Model 2</b>

```{r model2coef, echo = FALSE, warning = FALSE}
table1 = summary(step_model2)
kable(as.data.frame(table1["coefficients"]))
```

From the second model, I could clearly see that danceability, energy, liveness, loudness, mode, speechiness and key 6 have positive effects on streams, which means the more these factors used in the song, the more people the song will be played. Also, I intend to pay attention to release duration of the track . The longer the release duration is, the song will be played by less people, which means users prefer to play latest songs on.

<b> Figure3: Partial Dependence Plot (Model 5) </b>

```{r pdp, echo = FALSE, warning = FALSE}
# partial dependence plot: temp
p1 = bag_model %>%
  partial(pred.var = "danceability") %>% autoplot
# partial dependence plot: temp
p2 = bag_model %>%
  partial(pred.var = "energy") %>% autoplot
# partial dependence plot: hour
p3 = bag_model %>%
  partial(pred.var = "liveness") %>% autoplot
# partial dependence plot: day
p4 = bag_model %>%
  partial(pred.var = "loudness") %>% autoplot
# partial dependence plot: PC1
p5 = bag_model %>%
  partial(pred.var = "speechiness") %>% autoplot
# partial dependence plot: PC5
p6 = bag_model %>%
  partial(pred.var = "key6") %>% autoplot
multiplot(p1, p2, p3, p4, p5, p6, cols=2)
```

Last but not the least, I plot the partial dependence for each variable contained in the fifth model, and the results seem similar to those derived from the second model, which guarantee the robustness of results. In conclusion, both selected linear model(the second model) and the decision tree model(the fifth model) provided me with similar results.

## PCA and Clustering

###  General methodologies

In this section I would like to segment the 1,502 songs into groups with similar features in order to recommend to listeners who share the same interests/taste. For the reason of reducing unnecessary noises and computations, I first reduced the initial 25 variables by PCA. Next, I clustered them into groups with similar principle components, and based on the features in each principal component and the actual songs in each cluster, I were able to describe them in secular terminologies such as "genre".

```{r cluster_steup, echo = FALSE, warning = FALSE}
Clean_data <- subset(sounddata_2019, select = -c(releaseDate, artist_name, album_name, explicit, is_local, name, popularity, key0, key, time_signature, time_signature1, explicitFALSE, explicitTRUE, Streams, relseaseDuration))
# Center/scale the data
Clean_data_scaled <- scale(Clean_data, center=TRUE, scale=TRUE)
N = nrow(Clean_data_scaled)
# correlation
cor=cor(Clean_data_scaled)
```

### Part 1: PCA

In this part, I would like to use PCA to balance between the amount of computation load and explanatory variability, while eliminating as much noise as possible from the data. After demeaning and scaling of the data with standard deviation, I calculated the the loading matrix/scores matrix in order to derive the proportion of variance explained (PVE) and decide the number of principal components needed.

<b>Table 4: PCA Components</b>

```{r PCA_table, echo = FALSE, warning = FALSE}
# PCA
pca = prcomp(x = Clean_data_scaled,scale=TRUE)
loadings = pca$rotation
scores = pca$x
# PVE
VE = pca$sdev^2
PVE = VE / sum(VE) %>% round(4)
CP = c()
ID = c()
for (i in c(1:length(PVE))) {
  ID = c(ID, paste("PC", i, sep = ""))
  CP = c(CP, round(sum(PVE[1:i]),4))
}
summary_table = cbind(ID, VE, PVE, CP)
colnames(summary_table)<- c("ID", "Standard deviation", "Proportion of Variance", "Cumulative Proportion")
kable(summary_table)
```

Table 4 reports that the first 20 principle components explain more than 90% of the variability. and hence I believe that these 20 principle components would keep the computation load low and eliminate some of the noises, while keeping the majority of the variability. Clustering would further group the songs based on these 20 principle components.

### Part 2: Clustering

K-means++ clustering was used to determine the market segments. 3 types of supporting analysis were used to help me determine the number of K (centroids): Elbow plot(SSE), CH index and Gap statistics.

<b>Figure 4: SSE Grid vs K</b>

```{r K-grid, echo = FALSE, warning = FALSE}
pca_result = as.data.frame(pca$x)
pca_top_data <- subset(pca_result, select = -c(21:25))
#K-grid **15
k_grid = seq(2, 20, by=1)
SSE_grid = foreach(k = k_grid, .combine='c') %do% {
  cluster_k = kmeans(pca_top_data, k, nstart=50)
  cluster_k$tot.withinss
}
plot(k_grid, SSE_grid, xlab="K",ylab="SSE Grid")
```

<b>Figure5: CH Grid vs K</b>

```{r CH-grid, echo = FALSE, warning = FALSE}
#CH-grid to find the optimal K  **16
CH_grid = foreach(k = k_grid, .combine='c') %do% {
  cluster_k = kmeans(pca_top_data, k, nstart=50)
  W = cluster_k$tot.withinss
  B = cluster_k$betweenss
  CH = (B/W)*((N-k)/(k-1))
  CH
}
plot(k_grid, CH_grid, xlab="K",
     ylab="CH Grid")
```

<b> Figure6: Gap vs K </b>

```{r Gap, echo = FALSE, warning = FALSE}
#Gap statistics **4
Market_gap = clusGap(pca_top_data, FUN = kmeans, nstart = 40, K.max = 20, B = 10)
plot(Market_gap)
```

As shown above, both Elbow plot and CH index returned K=16 and Gap statistics returned K=4. Clustering 16 segments would not show the distinct differences among them as I now only have 20 principle components to allocate. So I selected K=4 as my anchor and explored the nearby Ks to see which one provides me with the best explanation for each cluster. For best explanation, I considered the following 2 categories.

- Clusters that have songs with clear and unique distribution in any of the 20 features.

- Clusters that have songs with clear genre by their artist name and actual music.(I have played a considerable quantity of sample size from each cluster on video music providers such as YouTube, for confirmation)

As the result, I eventually picked K = 5.

#### Catrgory 1: Song market segments breakdown by distribution of features

After 5 clusters were determined, first I reversed the principle components into the original features to determine cluster characteristics. Then I showed some of the cluster identifiable distributions and the summary of each cluster below.


```{r K_means, echo = FALSE, warning = FALSE}
# k-means analysis
clust2 = kmeanspp(pca_top_data, k=5, nstart=50)
cluster_result2 = as.data.frame(clust2[1])
cluster_result2 <- cbind(cluster_result2,sounddata_2019$artist_name,sounddata_2019$name)
#xtabs(~1+cluster_result2$cluster)
```

<b> Figure 7: cluster identifiable distributions </b>

```{r PC1, echo = FALSE, warning = FALSE}
#PC1
XX = subset(sounddata_2019,select = c(acousticness, time_signature3, time_signature5, instrumentalness, time_signature1))
ggpairs(XX,aes(col = as.factor(cluster_result2$cluster), alpha = 0.8))
```

```{r PC2, echo = FALSE, warning = FALSE}
#PC2
XX = subset(sounddata_2019,select = c(energy, loudness, time_signature5, time_signature3, liveness))
ggpairs(XX,aes(col = as.factor(cluster_result2$cluster), alpha = 0.8))
```

```{r PC3, echo = FALSE, warning = FALSE}
#PC3
XX = subset(sounddata_2019,select = c(speechiness, danceability, time_signature5, key11, key1))
ggpairs(XX,aes(col = as.factor(cluster_result2$cluster), alpha = 0.8))
```

- Cluster 1: High in energy, high in loudness, high danceability, low speechiness, considerate amount of G key, low acousticness

- Cluster 2: Many 5 quarter time signature songs, high in energy

- Cluster 3: Many songs with high energy, high on loudness

- Cluster 4: Many songs with high on loudness, high danceability, considerable amount of B flat key

- Cluster 5: Many 3 quarter time signature songs, low speechiness

#### Category 2: Song market segments breakdown by genre

Since I have the full list of song names and artist names available in each cluster, I could actually listen to the songs and categorize them manually by the music genre standard as in pop, rock, rap, etc. If my cluster characteristics determined by K-means++ show close resemblance of the music genre, then the recommendation system could be effective, at least to the extent of traditional music listeners with distinct preference over specific genre.

- Cluster 1: Many songs with electronically altered/amplified sounds, very rhythmic, but genre varying from pop to rap to country, etc. Typical examples would be Suge (Yea Yea) by DaBaby, Caro by Bad Bunny and Spy Kid by Chief Keef & Zaytoven.

- Cluster 2: Indeed many songs with 5/4 time signature, high energy and rhythmic, but clearly sets apart different vibe compared cluster 1, perhaps due to the different time signature. Typical examples would be Higher by DJ Khaled, Safety, and That's Mine by GASHI.

- Cluster 3: Genre varies a lot in this cluster, as shown in the very different artists such as Drake, Kendrick Lamar, Taylor Swift, XXXTENTACION and Queen. I did realize that out of the many rap songs in this cluster, most of them were the slower ones. For example, Me! by Taylor Swift and Who Needs Love by Trippie Redd.

- Cluster 4: Songs in B flat key stands out, such as Midnight In Prague by Lil Xan and Say Something by Justin Timberlake, which make this cluster a different vibe than others.

- Cluster 5: Many indie and pop songs with long vowel sounds, typical examples would be When We Were Young by Hollow Coves, Hay-on-Wye by Matthew Frederick and I Keep on Telling Myself by Dve Thomas Junior.

#### Trend in Popularity

I also calculated the total streams of different song clusters by time. The following graph demonstrates the trend in the total streams of different categories.

<b> Figure8: trend in the Total Streams </b>

```{r trend, echo = FALSE, warning = FALSE}
#row.names(cluster_result2)
cluster_result2_withID <- cbind(cluster_result2,row.names(cluster_result2))
# calculating cluster quantity trend by week
colnames(cluster_result2_withID)[4] <-"song_id"
colnames(sounddata_2019_weekly)[7] <-"song_id"
sounddata_2019_weekly$cluster <- cluster_result2_withID$cluster[match(sounddata_2019_weekly$song_id, cluster_result2_withID$song_id)]
Trend = sounddata_2019_weekly %>% group_by(Date, cluster) %>%  summarise(StreamNum = sum(Streams))
Pic = ggplot(data = Trend, mapping = aes(x = as.Date(Trend$Date), y = StreamNum, col = as.factor(cluster)))+
  geom_point()+
  geom_line(aes(group = as.factor(cluster)))+
  scale_x_date(date_breaks = "1 month")+
  theme(axis.text.x = element_text(angle = 90))+
  labs(x = "Weeks", y = "Streams")
Pic
```

From this graph it is demonstrated that the stream of five types of songs does not change too much in a year. Cluster 4 music has more streams overall, due to the fact that there are more songs in this categories. There is a peak in the end of April in 2019 for cluster 4, and then the streams goes back to normal. From this graph I can also see that at the end of the year cluster 4 music is not as popular as in the middle of the year, but type 5 music becomes more and more popular, especially in June and the end of the year. The popularity of cluster 1, cluster 2 and cluster 3 music doesn't vary too much throughout the whole year.

## Conclusion

In each age, the popularity of songs reflects people's preference over different music, which may also differ from each era. To predict the success of a song, taking the contemporaneous music preference into account is of significance. In streaming era, users are insane about music with elements of danceability, energy, liveness and so on. It seems that they are more likely to pursue the latest music. As a result, in order to predict the song's popularity trend in 2020, the first thing we need to do is to gather the information of users' music preference next year.

Traditional music listeners explore songs by specific genre and artists. This confirmation bias, typically nurtured through years of artificial genre segmentation by media and artist reputation, could limit listeners from the songs that they really want to be exposed to. The question of "why are we attracted to certain songs" is a philosophical discussion that is beyond the scope of our project here, but given the data from Pandora and the clustering method, I perhaps to show that key, time signature and speed of the songs are some of the contributing factors to the inner biological working of what to like and dislike. Then, my basic recommendation system, most likely already used by streaming music service providers like Pandora, Spotify, Apple Music, etc., could recommend songs not by mere genre and artist names, but also by specific keys and time signatures each listener is attracted to, subconsciously.
